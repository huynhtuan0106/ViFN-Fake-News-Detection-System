# ğŸ‡»ğŸ‡³ Vietnamese Fake News Detection System

## ğŸš€ BERT/PhoBERT Fine-tuning cho PhÃ¡t hiá»‡n Tin giáº£ Tiáº¿ng Viá»‡t

Há»‡ thá»‘ng phÃ¡t hiá»‡n tin giáº£ tiáº¿ng Viá»‡t sá»­ dá»¥ng **BERT/PhoBERT fine-tuning** vá»›i **multimodal fusion**, **domain regularization** vÃ  **SMOTETomek data balancing**.

---

## ğŸ¯ **Tá»”NG QUAN Há»† THá»NG**

âœ… **ÄÃ£ thá»±c hiá»‡n:**
- âœ… Fine-tune PhoBERT/BERT end-to-end cho tiáº¿ng Viá»‡t
- âœ… Multimodal fusion (title + summary + content + domain features)
- âœ… Domain regularization Ä‘á»ƒ trÃ¡nh domain overfitting  
- âœ… 3 fusion strategies: Concat, Attention, Gated
- âœ… So sÃ¡nh PhoBERT vs Multilingual BERT
- âœ… SMOTETomek data balancing (65:35 ratio)
- âœ… Mixed precision training (AMP) Ä‘á»ƒ tiáº¿t kiá»‡m GPU memory
- âœ… Comprehensive evaluation vÃ  visualization

---

## ğŸ—ï¸ **KIáº¾N TRÃšC Há»† THá»NG**

```
ğŸ“Š Data Loading & SMOTETomek Balancing
       â†“
ğŸ¤– PhoBERT Tokenization (Title, Summary, Content)
       â†“
ğŸ”¥ Fine-tuned BERT Embedders (Separate for each modality)
       â†“
ğŸŒ Domain Features Processing + Regularization
       â†“
ğŸ”€ Multimodal Fusion (Attention/Concat/Gated)
       â†“
ğŸ¯ Classification Head (2 classes: Real/Fake)
       â†“
ğŸ“ˆ Results & Comprehensive Evaluation
```

---

## ğŸ“ **Cáº¤U TRÃšC FILES VÃ€ VAI TRÃ’**

### **ğŸ¯ Core Training Files**

| File | Vai trÃ² | MÃ´ táº£ |
|------|---------|-------|
| `main.py` | ğŸšª **Main entry point** | Äiá»ƒm khá»Ÿi cháº¡y chÃ­nh, chá»n experiment type |
| `bert_training.py` | ğŸ‹ï¸ **Single model training** | Training logic cho 1 model vá»›i cáº¥u hÃ¬nh specific |
| `main_bert_experiment.py` | ğŸ”¬ **Model comparison** | So sÃ¡nh multiple BERT models/fusion strategies |
| `bert_fine_tuner.py` | ğŸ¤– **Model architectures** | Äá»‹nh nghÄ©a multimodal BERT fusion models |
| `bert_dataset.py` | ğŸ“Š **BERT dataset handling** | DataLoader vÃ  collate functions cho BERT |

### **ğŸ“š Data Processing Files**

| File | Vai trÃ² | MÃ´ táº£ |
|------|---------|-------|
| `data_loader.py` | ğŸ“¥ **Data loading & preprocessing** | Load CSV, text processing, domain features |
| `data_balancer.py` | âš–ï¸ **SMOTETomek balancing** | CÃ¢n báº±ng dá»¯ liá»‡u 65:35 ratio |

### **âš™ï¸ Configuration & Utilities**

| File | Vai trÃ² | MÃ´ táº£ |
|------|---------|-------|
| `config.py` | âš™ï¸ **System configuration** | GPU settings, BERT config, training parameters |
| `results_saver.py` | ğŸ’¾ **Results management** | LÆ°u metrics, plots, comprehensive reports |

### **ğŸ“‹ Documentation**

| File | Vai trÃ² | MÃ´ táº£ |
|------|---------|-------|
| `README.md` | ğŸ“– **Documentation** | HÆ°á»›ng dáº«n sá»­ dá»¥ng vÃ  cáº¥u trÃºc há»‡ thá»‘ng |
| `requirements_updated.txt` | ğŸ“¦ **Dependencies** | Python packages cáº§n thiáº¿t |

---

## ğŸš€ **CÃCH Sá»¬ Dá»¤NG**

### **BÆ°á»›c 1: Setup Environment**

```bash
# Táº¡o virtual environment
python -m venv fake_news_env
source fake_news_env/bin/activate  # Linux/Mac
# fake_news_env\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements_updated.txt
```

### **BÆ°á»›c 2: Cáº¥u hÃ¬nh GPU (Quan trá»ng!)**

```bash
# Kiá»ƒm tra GPU vÃ  Ä‘á» xuáº¥t cáº¥u hÃ¬nh tá»‘i Æ°u
python gpu_optimization_guide.py
```

**CÃ¡c cáº¥u hÃ¬nh Ä‘Æ°á»£c Ä‘á» xuáº¥t:**
- **4GB GPU**: batch_size=2, freeze 6 layers
- **6GB GPU**: batch_size=4, freeze 3 layers  
- **8GB GPU**: batch_size=8, no freezing (recommended)
- **12GB+ GPU**: batch_size=16+, full performance

### **BÆ°á»›c 3: Cáº­p nháº­t Ä‘Æ°á»ng dáº«n dá»¯ liá»‡u**

Chá»‰nh sá»­a `main.py` dÃ²ng 125-126:

```python
REAL_FILE_PATH = "path/to/your/real_articles.csv"
FAKE_FILE_PATH = "path/to/your/fake_articles.csv"
```

**Format dá»¯ liá»‡u cáº§n thiáº¿t:**
- `title_processed`: TiÃªu Ä‘á» Ä‘Ã£ xá»­ lÃ½
- `summary_processed`: TÃ³m táº¯t Ä‘Ã£ xá»­ lÃ½  
- `content_processed`: Ná»™i dung Ä‘Ã£ xá»­ lÃ½
- `label`: 0 (Real) / 1 (Fake)
- `domain`: TÃªn miá»n (optional)

### **BÆ°á»›c 4: Cháº¡y Training**

```bash
fake_news_env\Scripts\Activate.ps1
python main.py
```

**Chá»n experiment type:**
- `1`: **Single Model Training** - Train 1 model vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u
- `2`: **Model Comparison** - So sÃ¡nh 5 configurations khÃ¡c nhau

---

## ğŸ§ª **CÃC THá»°C NGHIá»†M**

### **1. Single Model Training**

Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh (khuyáº¿n nghá»‹):

```python
{
    'bert_model': 'vinai/phobert-base',
    'fusion_type': 'attention',
    'use_domain': True,
    'num_epochs': 3,
    'batch_size': 8,  # TÃ¹y thuá»™c GPU
    'learning_rate': 2e-5,
    'balance_strategy': 'smotetomek'
}
```

### **2. Model Comparison**

So sÃ¡nh 5 configurations tá»± Ä‘á»™ng:

1. **PhoBERT_Concat**: Concat fusion
2. **PhoBERT_Attention**: Attention fusion + domain regularization â­
3. **PhoBERT_Gated**: Gated fusion
4. **MultiBERT_Attention**: Multilingual BERT comparison
5. **PhoBERT_Attention_NoDomain**: Test domain impact

---

## ğŸ¯ **FUSION STRATEGIES**

### **1. Attention Fusion** â­ (Tá»‘t nháº¥t)
```python
# Learnable attention weights vá»›i domain regularization
weights = F.softmax(attention_weights, dim=0)
# Giá»›i háº¡n domain weight â‰¤ 15% Ä‘á»ƒ trÃ¡nh overfitting
if domain_weight > 0.15:
    penalty = (domain_weight - 0.15) ** 2
```

### **2. Concat Fusion**
```python
# ÄÆ¡n giáº£n ná»‘i cÃ¡c embeddings
fused = torch.cat([title_emb, summary_emb, content_emb, domain_emb], dim=1)
```

### **3. Gated Fusion**
```python
# Gating mechanism Ä‘á»ƒ kiá»ƒm soÃ¡t information flow
gate_weights = sigmoid(gate_network(concat_embeddings))
fused = concat_embeddings * gate_weights
```

---

## ğŸ® **GPU OPTIMIZATION & MEMORY MANAGEMENT**

### **âš¡ Memory Optimization Features:**

1. **ğŸ› ï¸ Enhanced Memory Management**:
   - âœ… Automatic GPU cache clearing before evaluation
   - âœ… CPU fallback for model loading when GPU memory insufficient
   - âœ… Gradient checkpointing for memory efficiency
   - âœ… Expandable CUDA memory segments
   - âœ… Periodic memory cleanup during training

2. **ğŸ”§ Configuration Optimizations**:
   - âœ… Reduced max_length from 128 â†’ 96 tokens
   - âœ… Increased gradient accumulation steps: 8 â†’ 16
   - âœ… Mixed precision training (AMP)
   - âœ… Smart batch size recommendations by GPU memory

### **Key Factors Affecting GPU Memory:**

1. **Batch Size** (Quan trá»ng nháº¥t):
   - Linear scaling: batch_size x2 â†’ memory x2
   - Khuyáº¿n nghá»‹: báº¯t Ä‘áº§u vá»›i 1, tÄƒng dáº§n theo GPU memory

2. **Sequence Length**:
   - Quadratic scaling: length x2 â†’ memory x4
   - 96 tokens: cÃ¢n báº±ng tá»‘t cho 4GB GPU

3. **ğŸ†• Memory Error Handling**:
   ```python
   # Automatic fallback to CPU if GPU memory insufficient
   try:
       checkpoint = torch.load(model_path, map_location='cpu')
       model.load_state_dict(checkpoint['model_state_dict'])
       model = model.to(DEVICE)
   except torch.cuda.OutOfMemoryError:
       # Fallback to CPU evaluation
       model = model.cpu()
   ```

### **ğŸ§ª Test Memory Optimizations:**

```bash
# Test cÃ¡c cáº£i tiáº¿n memory
python test_memory_optimizations.py
```

### **âš™ï¸ Environment Variables for Memory:**

```bash
# Thiáº¿t láº­p environment variable Ä‘á»ƒ tá»‘i Æ°u memory
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### **ğŸ“Š GPU Memory Monitoring:**

Há»‡ thá»‘ng tá»± Ä‘á»™ng monitor vÃ  bÃ¡o cÃ¡o:
- GPU memory usage trÆ°á»›c/sau má»—i epoch
- Memory cleanup status
- Fallback warnings khi cáº§n thiáº¿t

3. **Model Count**:
   - 3 BERT models (title, summary, content)
   - Shared weights Ä‘á»ƒ giáº£m memory

4. **Optimization Techniques**:
   - **Mixed Precision (AMP)**: -40% memory, +20% speed
   - **Gradient Checkpointing**: -30% memory, -20% speed
   - **Freeze Layers**: -50% memory, +30% speed

---

## ğŸ“Š **Káº¾T QUá»¢ MONG Äá»¢I**

### **Performance Benchmarks:**
- **Accuracy**: 85-92%
- **F1-Score (Fake News)**: 83-90%
- **PhoBERT** > **Multilingual BERT** cho tiáº¿ng Viá»‡t
- **Attention Fusion** thÆ°á»ng tá»‘t nháº¥t

### **Training Time:**
- **8GB GPU**: ~2-3 hours (batch_size=8)
- **12GB GPU**: ~1-2 hours (batch_size=16)
- **CPU only**: ~8-12 hours (khÃ´ng khuyáº¿n nghá»‹)

### **Output Files:**
```
results_single_YYYYMMDD_HHMMSS/
â”œâ”€â”€ bert_attention_best.pt              # Best model weights
â”œâ”€â”€ bert_attention_comprehensive_results.png  # Evaluation plots
â”œâ”€â”€ bert_attention_detailed_metrics.json      # Detailed metrics
â”œâ”€â”€ bert_attention_summary_report.txt         # Human-readable report
â”œâ”€â”€ training_history.png                      # Training curves
â””â”€â”€ experiment_config.json                    # Experiment settings
```

---

## âš¡ **PERFORMANCE OPTIMIZATIONS**

### **Memory Optimizations:**
```python
# Trong config.py
GPU_CONFIG = {
    'mixed_precision': True,      # Enable AMP
    'pin_memory': True,           # Faster data transfer
    'prefetch_factor': 2,         # Background data loading
    'persistent_workers': True    # Reuse worker processes
}
```

### **Speed Optimizations:**
```python
# BERT caching
torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes
torch.backends.cudnn.deterministic = False  # Allow faster algorithms
```

---

## ğŸš¨ **TROUBLESHOOTING**

### **CUDA Out of Memory:**
```bash
# 1. Giáº£m batch size
batch_size = 4  # instead of 8

# 2. Enable gradient checkpointing
gradient_checkpointing = True

# 3. Freeze BERT layers
freeze_bert_layers = 6

# 4. Reduce sequence length
max_length = 128  # instead of 256
```

### **Slow Training:**
```bash
# 1. Increase batch size (if memory allows)
batch_size = 16

# 2. Use mixed precision
mixed_precision = True

# 3. Increase num_workers
num_workers = 4
```

### **Poor Performance:**
```bash
# 1. Check data balancing
python -c "from data_balancer import *; check_balance_distribution()"

# 2. Verify domain regularization
domain_penalty_weight = 0.5

# 3. Try different fusion types
fusion_type = 'attention'  # usually best
```

---

## ğŸ”§ **CUSTOM CONFIGURATIONS**

### **For Different Hardware:**

**Low Memory (â‰¤6GB):**
```python
config = {
    'batch_size': 4,
    'max_length': 128,
    'mixed_precision': True,
    'freeze_bert_layers': 6,
    'gradient_checkpointing': True
}
```

**High Memory (â‰¥16GB):**
```python
config = {
    'batch_size': 32,
    'max_length': 512,
    'mixed_precision': False,
    'freeze_bert_layers': 0,
    'gradient_checkpointing': False
}
```

### **For Different Datasets:**

**Small Dataset (<5K samples):**
```python
config = {
    'num_epochs': 5,
    'balance_strategy': 'weighted',
    'learning_rate': 1e-5  # Lower LR
}
```

**Large Dataset (>50K samples):**
```python
config = {
    'num_epochs': 2,
    'balance_strategy': 'smotetomek',
    'learning_rate': 3e-5  # Higher LR
}
```

---

## ğŸ“ˆ **MONITORING & DEBUGGING**

### **GPU Memory Monitoring:**
```python
# Trong training loop
if batch_idx % 100 == 0:
    print(f"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")
```

### **Training Progress:**
- Loss curves Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng
- Validation metrics má»—i epoch
- Best model checkpoint tá»± Ä‘á»™ng

### **Performance Analysis:**
```bash
# Xem detailed results
python -c "from results_saver import *; analyze_results('results_dir')"
```

---

## ğŸ“ **SUPPORT & FAQ**

### **Common Issues:**

**Q: GPU khÃ´ng Ä‘Æ°á»£c detect?**
```bash
# Check CUDA installation
python -c "import torch; print(torch.cuda.is_available())"
```

**Q: Training bá»‹ killed?**
```bash
# Reduce batch size hoáº·c enable swapping
batch_size = 2
```

**Q: Accuracy tháº¥p?**
```bash
# Check data quality vÃ  balance ratio
python data_balancer.py
```

### **Best Practices:**
1. ğŸ¯ LuÃ´n báº¯t Ä‘áº§u vá»›i cáº¥u hÃ¬nh conservative
2. ğŸ’¾ Monitor GPU memory usage
3. ğŸ“Š Validate trÃªn multiple runs
4. ğŸ”„ Save checkpoints thÆ°á»ng xuyÃªn
5. ğŸ“ˆ Track experiments vá»›i tensorboard/wandb

---

## ğŸ¯ **CONCLUSION**

### **Recommended Workflow:**
1. **Setup**: Check GPU â†’ Install dependencies
2. **Config**: Run `gpu_optimization_guide.py` 
3. **Data**: Prepare CSV vá»›i format Ä‘Ãºng
4. **Training**: Start vá»›i single model (option 1)
5. **Optimize**: TÄƒng batch size gradually
6. **Compare**: Cháº¡y model comparison (option 2)
7. **Deploy**: Use best model cho inference

### **Key Success Factors:**
- âœ… **PhoBERT** tá»‘i Æ°u cho tiáº¿ng Viá»‡t
- âœ… **Attention fusion** vá»›i domain regularization
- âœ… **SMOTETomek balancing** 65:35 ratio
- âœ… **Mixed precision training** Ä‘á»ƒ tiáº¿t kiá»‡m memory
- âœ… **Proper GPU configuration** theo hardware

---

**ğŸ“ Note**: Codebase nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho Vietnamese fake news detection vá»›i BERT fine-tuning. Má»i phÆ°Æ¡ng phÃ¡p TF-IDF cÅ© Ä‘Ã£ Ä‘Æ°á»£c loáº¡i bá» hoÃ n toÃ n.

