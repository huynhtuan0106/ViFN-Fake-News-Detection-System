# üáªüá≥ Vietnamese Fake News Detection System

## üöÄ BERT/PhoBERT Fine-tuning cho Ph√°t hi·ªán Tin gi·∫£ Ti·∫øng Vi·ªát

H·ªá th·ªëng ph√°t hi·ªán tin gi·∫£ ti·∫øng Vi·ªát s·ª≠ d·ª•ng **BERT/PhoBERT fine-tuning** v·ªõi **multimodal fusion**, **domain regularization** v√† **SMOTETomek data balancing**.

---

## üéØ **T·ªîNG QUAN H·ªÜ TH·ªêNG**

‚úÖ **ƒê√£ th·ª±c hi·ªán:**
- ‚úÖ Fine-tune PhoBERT/BERT end-to-end cho ti·∫øng Vi·ªát
- ‚úÖ Multimodal fusion (title + summary + content + domain features)
- ‚úÖ Domain regularization ƒë·ªÉ tr√°nh domain overfitting  
- ‚úÖ 3 fusion strategies: Concat, Attention, Gated
- ‚úÖ So s√°nh PhoBERT vs Multilingual BERT
- ‚úÖ SMOTETomek data balancing (65:35 ratio)
- ‚úÖ Mixed precision training (AMP) ƒë·ªÉ ti·∫øt ki·ªám GPU memory
- ‚úÖ Comprehensive evaluation v√† visualization

---

## üèóÔ∏è **KI·∫æN TR√öC H·ªÜ TH·ªêNG**

```
üìä Data Loading & SMOTETomek Balancing
       ‚Üì
ü§ñ PhoBERT Tokenization (Title, Summary, Content)
       ‚Üì
üî• Fine-tuned BERT Embedders (Separate for each modality)
       ‚Üì
üåê Domain Features Processing + Regularization
       ‚Üì
üîÄ Multimodal Fusion (Attention/Concat/Gated)
       ‚Üì
üéØ Classification Head (2 classes: Real/Fake)
       ‚Üì
üìà Results & Comprehensive Evaluation
```

---

## üìÅ **C·∫§U TR√öC FILES V√Ä VAI TR√í**

### **üéØ Core Training Files**

| File | Vai tr√≤ | M√¥ t·∫£ |
|------|---------|-------|
| `main.py` | üö™ **Main entry point** | ƒêi·ªÉm kh·ªüi ch·∫°y ch√≠nh, ch·ªçn experiment type |
| `bert_training.py` | üèãÔ∏è **Single model training** | Training logic cho 1 model v·ªõi c·∫•u h√¨nh specific |
| `main_bert_experiment.py` | üî¨ **Model comparison** | So s√°nh multiple BERT models/fusion strategies |
| `bert_fine_tuner.py` | ü§ñ **Model architectures** | ƒê·ªãnh nghƒ©a multimodal BERT fusion models |
| `bert_dataset.py` | üìä **BERT dataset handling** | DataLoader v√† collate functions cho BERT |

### **üìö Data Processing Files**

| File | Vai tr√≤ | M√¥ t·∫£ |
|------|---------|-------|
| `data_loader.py` | üì• **Data loading & preprocessing** | Load CSV, text processing, domain features |
| `data_balancer.py` | ‚öñÔ∏è **SMOTETomek balancing** | C√¢n b·∫±ng d·ªØ li·ªáu 65:35 ratio |

### **‚öôÔ∏è Configuration & Utilities**

| File | Vai tr√≤ | M√¥ t·∫£ |
|------|---------|-------|
| `config.py` | ‚öôÔ∏è **System configuration** | GPU settings, BERT config, training parameters |
| `results_saver.py` | üíæ **Results management** | L∆∞u metrics, plots, comprehensive reports |

### **üìã Documentation**

| File | Vai tr√≤ | M√¥ t·∫£ |
|------|---------|-------|
| `README.md` | üìñ **Documentation** | H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng v√† c·∫•u tr√∫c h·ªá th·ªëng |
| `requirements_updated.txt` | üì¶ **Dependencies** | Python packages c·∫ßn thi·∫øt |

---

## üöÄ **C√ÅCH S·ª¨ D·ª§NG**

### **B∆∞·ªõc 1: Setup Environment**

```bash
# T·∫°o virtual environment
python -m venv fake_news_env
source fake_news_env/bin/activate  # Linux/Mac
# fake_news_env\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements_updated.txt
```

### **B∆∞·ªõc 2: C·∫•u h√¨nh GPU (Quan tr·ªçng!)**

```bash
# Ki·ªÉm tra GPU v√† ƒë·ªÅ xu·∫•t c·∫•u h√¨nh t·ªëi ∆∞u
python gpu_optimization_guide.py
```

**C√°c c·∫•u h√¨nh ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:**
- **4GB GPU**: batch_size=2, freeze 6 layers
- **6GB GPU**: batch_size=4, freeze 3 layers  
- **8GB GPU**: batch_size=8, no freezing (recommended)
- **12GB+ GPU**: batch_size=16+, full performance

### **B∆∞·ªõc 3: C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu**

Ch·ªânh s·ª≠a `main.py` d√≤ng 125-126:

```python
REAL_FILE_PATH = "path/to/your/real_articles.csv"
FAKE_FILE_PATH = "path/to/your/fake_articles.csv"
```

**Format d·ªØ li·ªáu c·∫ßn thi·∫øt:**
- `title_processed`: Ti√™u ƒë·ªÅ ƒë√£ x·ª≠ l√Ω
- `summary_processed`: T√≥m t·∫Øt ƒë√£ x·ª≠ l√Ω  
- `content_processed`: N·ªôi dung ƒë√£ x·ª≠ l√Ω
- `label`: 0 (Real) / 1 (Fake)
- `domain`: T√™n mi·ªÅn (optional)

### **B∆∞·ªõc 4: Ch·∫°y Training**

```bash
fake_news_env\Scripts\Activate.ps1
python main.py
```

**Ch·ªçn experiment type:**
- `1`: **Single Model Training** - Train 1 model v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u
- `2`: **Model Comparison** - So s√°nh 5 configurations kh√°c nhau

---

## üß™ **C√ÅC TH·ª∞C NGHI·ªÜM**

### **1. Single Model Training**

C·∫•u h√¨nh m·∫∑c ƒë·ªãnh (khuy·∫øn ngh·ªã):

```python
{
    'bert_model': 'vinai/phobert-base',
    'fusion_type': 'attention',
    'use_domain': True,
    'num_epochs': 3,
    'batch_size': 8,  # T√πy thu·ªôc GPU
    'learning_rate': 2e-5,
    'balance_strategy': 'smotetomek'
}
```

### **2. Model Comparison**

So s√°nh 5 configurations t·ª± ƒë·ªông:

1. **PhoBERT_Concat**: Concat fusion
2. **PhoBERT_Attention**: Attention fusion + domain regularization ‚≠ê
3. **PhoBERT_Gated**: Gated fusion
4. **MultiBERT_Attention**: Multilingual BERT comparison
5. **PhoBERT_Attention_NoDomain**: Test domain impact

---

## üéØ **FUSION STRATEGIES**

### **1. Attention Fusion** ‚≠ê (T·ªët nh·∫•t)
```python
# Learnable attention weights v·ªõi domain regularization
weights = F.softmax(attention_weights, dim=0)
# Gi·ªõi h·∫°n domain weight ‚â§ 15% ƒë·ªÉ tr√°nh overfitting
if domain_weight > 0.15:
    penalty = (domain_weight - 0.15) ** 2
```

### **2. Concat Fusion**
```python
# ƒê∆°n gi·∫£n n·ªëi c√°c embeddings
fused = torch.cat([title_emb, summary_emb, content_emb, domain_emb], dim=1)
```

### **3. Gated Fusion**
```python
# Gating mechanism ƒë·ªÉ ki·ªÉm so√°t information flow
gate_weights = sigmoid(gate_network(concat_embeddings))
fused = concat_embeddings * gate_weights
```

---

## üéÆ **GPU OPTIMIZATION & MEMORY MANAGEMENT**

### **‚ö° Memory Optimization Features:**

1. **üõ†Ô∏è Enhanced Memory Management**:
   - ‚úÖ Automatic GPU cache clearing before evaluation
   - ‚úÖ CPU fallback for model loading when GPU memory insufficient
   - ‚úÖ Gradient checkpointing for memory efficiency
   - ‚úÖ Expandable CUDA memory segments
   - ‚úÖ Periodic memory cleanup during training

2. **üîß Configuration Optimizations**:
   - ‚úÖ Reduced max_length from 128 ‚Üí 96 tokens
   - ‚úÖ Increased gradient accumulation steps: 8 ‚Üí 16
   - ‚úÖ Mixed precision training (AMP)
   - ‚úÖ Smart batch size recommendations by GPU memory

### **Key Factors Affecting GPU Memory:**

1. **Batch Size** (Quan tr·ªçng nh·∫•t):
   - Linear scaling: batch_size x2 ‚Üí memory x2
   - Khuy·∫øn ngh·ªã: b·∫Øt ƒë·∫ßu v·ªõi 1, tƒÉng d·∫ßn theo GPU memory

2. **Sequence Length**:
   - Quadratic scaling: length x2 ‚Üí memory x4
   - 96 tokens: c√¢n b·∫±ng t·ªët cho 4GB GPU

3. **üÜï Memory Error Handling**:
   ```python
   # Automatic fallback to CPU if GPU memory insufficient
   try:
       checkpoint = torch.load(model_path, map_location='cpu')
       model.load_state_dict(checkpoint['model_state_dict'])
       model = model.to(DEVICE)
   except torch.cuda.OutOfMemoryError:
       # Fallback to CPU evaluation
       model = model.cpu()
   ```

### **üß™ Test Memory Optimizations:**

```bash
# Test c√°c c·∫£i ti·∫øn memory
python test_memory_optimizations.py
```

### **‚öôÔ∏è Environment Variables for Memory:**

```bash
# Thi·∫øt l·∫≠p environment variable ƒë·ªÉ t·ªëi ∆∞u memory
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### **üìä GPU Memory Monitoring:**

H·ªá th·ªëng t·ª± ƒë·ªông monitor v√† b√°o c√°o:
- GPU memory usage tr∆∞·ªõc/sau m·ªói epoch
- Memory cleanup status
- Fallback warnings khi c·∫ßn thi·∫øt

3. **Model Count**:
   - 3 BERT models (title, summary, content)
   - Shared weights ƒë·ªÉ gi·∫£m memory

4. **Optimization Techniques**:
   - **Mixed Precision (AMP)**: -40% memory, +20% speed
   - **Gradient Checkpointing**: -30% memory, -20% speed
   - **Freeze Layers**: -50% memory, +30% speed

---

## üìä **K·∫æT QU·ª¢ MONG ƒê·ª¢I**

### **Performance Benchmarks:**
- **Accuracy**: 85-92%
- **F1-Score (Fake News)**: 83-90%
- **PhoBERT** > **Multilingual BERT** cho ti·∫øng Vi·ªát
- **Attention Fusion** th∆∞·ªùng t·ªët nh·∫•t

### **Training Time:**
- **8GB GPU**: ~2-3 hours (batch_size=8)
- **12GB GPU**: ~1-2 hours (batch_size=16)
- **CPU only**: ~8-12 hours (kh√¥ng khuy·∫øn ngh·ªã)

### **Output Files:**
```
results_single_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ bert_attention_best.pt              # Best model weights
‚îú‚îÄ‚îÄ bert_attention_comprehensive_results.png  # Evaluation plots
‚îú‚îÄ‚îÄ bert_attention_detailed_metrics.json      # Detailed metrics
‚îú‚îÄ‚îÄ bert_attention_summary_report.txt         # Human-readable report
‚îú‚îÄ‚îÄ training_history.png                      # Training curves
‚îî‚îÄ‚îÄ experiment_config.json                    # Experiment settings
```

---

## ‚ö° **PERFORMANCE OPTIMIZATIONS**

### **Memory Optimizations:**
```python
# Trong config.py
GPU_CONFIG = {
    'mixed_precision': True,      # Enable AMP
    'pin_memory': True,           # Faster data transfer
    'prefetch_factor': 2,         # Background data loading
    'persistent_workers': True    # Reuse worker processes
}
```

### **Speed Optimizations:**
```python
# BERT caching
torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes
torch.backends.cudnn.deterministic = False  # Allow faster algorithms
```

---

## üö® **TROUBLESHOOTING**

### **CUDA Out of Memory:**
```bash
# 1. Gi·∫£m batch size
batch_size = 4  # instead of 8

# 2. Enable gradient checkpointing
gradient_checkpointing = True

# 3. Freeze BERT layers
freeze_bert_layers = 6

# 4. Reduce sequence length
max_length = 128  # instead of 256
```

### **Slow Training:**
```bash
# 1. Increase batch size (if memory allows)
batch_size = 16

# 2. Use mixed precision
mixed_precision = True

# 3. Increase num_workers
num_workers = 4
```

### **Poor Performance:**
```bash
# 1. Check data balancing
python -c "from data_balancer import *; check_balance_distribution()"

# 2. Verify domain regularization
domain_penalty_weight = 0.5

# 3. Try different fusion types
fusion_type = 'attention'  # usually best
```

---

## üîß **CUSTOM CONFIGURATIONS**

### **For Different Hardware:**

**Low Memory (‚â§6GB):**
```python
config = {
    'batch_size': 4,
    'max_length': 128,
    'mixed_precision': True,
    'freeze_bert_layers': 6,
    'gradient_checkpointing': True
}
```

**High Memory (‚â•16GB):**
```python
config = {
    'batch_size': 32,
    'max_length': 512,
    'mixed_precision': False,
    'freeze_bert_layers': 0,
    'gradient_checkpointing': False
}
```

### **For Different Datasets:**

**Small Dataset (<5K samples):**
```python
config = {
    'num_epochs': 5,
    'balance_strategy': 'weighted',
    'learning_rate': 1e-5  # Lower LR
}
```

**Large Dataset (>50K samples):**
```python
config = {
    'num_epochs': 2,
    'balance_strategy': 'smotetomek',
    'learning_rate': 3e-5  # Higher LR
}
```

---

## üìà **MONITORING & DEBUGGING**

### **GPU Memory Monitoring:**
```python
# Trong training loop
if batch_idx % 100 == 0:
    print(f"GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB")
```

### **Training Progress:**
- Loss curves ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông
- Validation metrics m·ªói epoch
- Best model checkpoint t·ª± ƒë·ªông

### **Performance Analysis:**
```bash
# Xem detailed results
python -c "from results_saver import *; analyze_results('results_dir')"
```

---

## üìû **SUPPORT & FAQ**

### **Common Issues:**

**Q: GPU kh√¥ng ƒë∆∞·ª£c detect?**
```bash
# Check CUDA installation
python -c "import torch; print(torch.cuda.is_available())"
```

**Q: Training b·ªã killed?**
```bash
# Reduce batch size ho·∫∑c enable swapping
batch_size = 2
```

**Q: Accuracy th·∫•p?**
```bash
# Check data quality v√† balance ratio
python data_balancer.py
```

### **Best Practices:**
1. üéØ Lu√¥n b·∫Øt ƒë·∫ßu v·ªõi c·∫•u h√¨nh conservative
2. üíæ Monitor GPU memory usage
3. üìä Validate tr√™n multiple runs
4. üîÑ Save checkpoints th∆∞·ªùng xuy√™n
5. üìà Track experiments v·ªõi tensorboard/wandb

---

## üéØ **CONCLUSION**

### **Recommended Workflow:**
1. **Setup**: Check GPU ‚Üí Install dependencies
2. **Config**: Run `gpu_optimization_guide.py` 
3. **Data**: Prepare CSV v·ªõi format ƒë√∫ng
4. **Training**: Start v·ªõi single model (option 1)
5. **Optimize**: TƒÉng batch size gradually
6. **Compare**: Ch·∫°y model comparison (option 2)
7. **Deploy**: Use best model cho inference

### **Key Success Factors:**
- ‚úÖ **PhoBERT** t·ªëi ∆∞u cho ti·∫øng Vi·ªát
- ‚úÖ **Attention fusion** v·ªõi domain regularization
- ‚úÖ **SMOTETomek balancing** 65:35 ratio
- ‚úÖ **Mixed precision training** ƒë·ªÉ ti·∫øt ki·ªám memory
- ‚úÖ **Proper GPU configuration** theo hardware

---

**üìù Note**: Codebase n√†y ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho Vietnamese fake news detection v·ªõi BERT fine-tuning. M·ªçi ph∆∞∆°ng ph√°p TF-IDF c≈© ƒë√£ ƒë∆∞·ª£c lo·∫°i b·ªè ho√†n to√†n.

